"""
Sistema de b√∫squeda mejorado que integra Graph RAG Avanzado con filtrado inteligente
para obtener 8-12 art√≠culos relevantes en lugar de 2-3.
"""

import time
from typing import Dict, Any, List, Optional
from collections import defaultdict

def search_query_with_enhanced_graph_rag(query: str, config: Dict[str, Any], 
                                        weaviate_client=None, neo4j_driver=None, 
                                        documents=None) -> List[Dict[str, Any]]:
    """
    B√∫squeda principal mejorada que integra Graph RAG Avanzado para obtener m√°s art√≠culos relevantes.
    Objetivo: 8-12 art√≠culos relevantes en lugar de 2-3.
    """
    print(f"\nüöÄ Procesando consulta con Graph RAG Avanzado v2: '{query}'")
    start_time = time.time()
    
    # 1. An√°lisis mejorado de la consulta
    print("üß† Realizando an√°lisis avanzado de consulta...")
    query_analysis = enhanced_query_analysis(query)
    
    print(f"   üìä Complejidad detectada: {query_analysis['query_complexity']:.2f}")
    if query_analysis['legal_entities']:
        entities = [e['type'] for e in query_analysis['legal_entities']]
        print(f"   ‚öñÔ∏è Entidades legales: {', '.join(entities)}")
    if query_analysis['urgency_indicators']:
        print(f"   üö® Urgencia detectada: {len(query_analysis['urgency_indicators'])} indicadores")
    
    # 2. Estrategia de b√∫squeda mejorada (m√°s agresiva)
    search_strategy = determine_enhanced_search_strategy(query_analysis, config)
    print(f"   üéØ Estrategia seleccionada: {search_strategy['name']}")
    
    # 3. Ejecutar b√∫squedas con l√≠mites m√°s altos
    all_results = []
    
    # A. Graph RAG Avanzado (PRIORIDAD M√ÅXIMA - M√°s resultados)
    if neo4j_driver and config.get("neo4j", {}).get("enabled", False):
        print("üï∏Ô∏è Ejecutando Graph RAG Avanzado...")
        try:
            # Importar la funci√≥n mejorada
            from .neo4j_utils import search_neo4j_enhanced
            
            graph_rag_results = search_neo4j_enhanced(
                neo4j_driver, query, 
                limit=search_strategy['graph_rag_limit']  # L√≠mite m√°s alto
            )
            
            # Aplicar boost basado en an√°lisis de consulta
            for result in graph_rag_results:
                result['score'] *= search_strategy['graph_rag_weight']
                # Marcar m√©todo si no est√° marcado
                if 'method' not in result or result['method'] in ['enhanced_seed', 'strong_relations', 'semantic_context', 'procedural_chains']:
                    result['method'] = 'advanced_graph_rag_v2'
            
            all_results.extend(graph_rag_results)
            print(f"   ‚úÖ Graph RAG Avanzado: {len(graph_rag_results)} resultados")
            
        except Exception as e:
            print(f"   ‚ùå Error en Graph RAG: {str(e)}")
    
    # B. B√∫squeda vectorial complementaria (aumentar l√≠mite)
    if weaviate_client and config.get("weaviate", {}).get("enabled", False):
        print("üîÆ Ejecutando b√∫squeda vectorial complementaria...")
        try:
            from .weaviate_utils import search_weaviate
            
            collection_name = config["weaviate"].get("collection_name", "ArticulosLegales")
            embedding_model = config["weaviate"].get("embedding_model", "paraphrase-multilingual-MiniLM-L12-v2")
            
            vector_results = search_weaviate(
                weaviate_client, collection_name, query,
                embedding_model=embedding_model, 
                top_n=search_strategy['vector_limit']  # L√≠mite aumentado
            )
            
            # Aplicar peso basado en estrategia
            for result in vector_results:
                result['score'] *= search_strategy['vector_weight']
                result['method'] = 'weaviate_vectorial'
            
            all_results.extend(vector_results)
            print(f"   ‚úÖ Vectorial: {len(vector_results)} resultados")
            
        except Exception as e:
            print(f"   ‚ùå Error en b√∫squeda vectorial: {str(e)}")
    
    # C. B√∫squeda sem√°ntica directa (si necesitamos m√°s resultados)
    if documents and len(all_results) < search_strategy['min_results']:
        print("üß≠ Ejecutando b√∫squeda sem√°ntica directa...")
        semantic_results = semantic_similarity_search(
            query, documents, 
            top_k=search_strategy['semantic_limit']
        )
        
        for result in semantic_results:
            result['score'] *= search_strategy['semantic_weight']
        
        all_results.extend(semantic_results)
        print(f"   ‚úÖ Sem√°ntica: {len(semantic_results)} resultados")
    
    print(f"üìä Total resultados antes de fusi√≥n: {len(all_results)}")
    
    # 4. Fusi√≥n inteligente y eliminaci√≥n de duplicados (mejorada)
    print("üîÑ Fusionando resultados...")
    unique_results = enhanced_deduplication(all_results)
    print(f"üìä Resultados √∫nicos: {len(unique_results)}")
    
    # 5. Re-scoring contextual mejorado
    print("üéØ Aplicando re-scoring contextual avanzado...")
    contextual_results = apply_enhanced_contextual_rescoring(unique_results, query_analysis, query)
    
    # 6. Filtro final de relevancia m√°s permisivo
    print("üîç Aplicando filtro final de relevancia...")
    final_results = enhanced_relevance_filter(contextual_results, query, threshold=0.05)
    
    # 7. Limitar resultados finales (m√°s generoso)
    top_n = max(config.get("retrieval", {}).get("top_n", 20), 15)  # M√≠nimo 15 resultados
    final_results.sort(key=lambda x: x.get('score', 0), reverse=True)
    limited_results = final_results[:top_n]
    
    end_time = time.time()
    print(f"‚ö° B√∫squeda completada en {end_time - start_time:.2f} segundos")
    print(f"üéâ Resultados finales: {len(limited_results)}")
    
    # Mostrar breakdown detallado de m√©todos
    method_breakdown = defaultdict(int)
    expansion_breakdown = defaultdict(int)
    
    for result in limited_results:
        method = result.get('method', 'unknown')
        method_breakdown[method] += 1
        
        expansion_level = result.get('expansion_level', 0)
        if expansion_level > 0:
            expansion_breakdown[f'nivel_{expansion_level}'] += 1
    
    print(f"üìà Breakdown por m√©todo: {dict(method_breakdown)}")
    if expansion_breakdown:
        print(f"üï∏Ô∏è Breakdown por expansi√≥n: {dict(expansion_breakdown)}")
    
    return limited_results

def determine_enhanced_search_strategy(query_analysis: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Estrategia de b√∫squeda mejorada que busca m√°s art√≠culos relevantes.
    """
    complexity = query_analysis['query_complexity']
    has_urgency = len(query_analysis['urgency_indicators']) > 0
    has_emotional_context = len(query_analysis['emotional_context']) > 0
    legal_entity_count = len(query_analysis['legal_entities'])
    
    # Estrategia para consultas complejas con alta urgencia
    if complexity > 0.7 or has_urgency:
        return {
            'name': 'high_priority_comprehensive_v2',
            'graph_rag_limit': 25,  # Aumentado de 20
            'vector_limit': 20,     # Aumentado de 15
            'semantic_limit': 15,
            'graph_rag_weight': 2.5,
            'vector_weight': 1.8,
            'semantic_weight': 1.2,
            'min_results': 15
        }
    
    # Estrategia para consultas con alto contenido emocional (despidos, discriminaci√≥n)
    elif has_emotional_context or complexity > 0.4:
        return {
            'name': 'rights_violation_focused_v2',
            'graph_rag_limit': 20,  # Aumentado de 15
            'vector_limit': 18,     # Aumentado de 12
            'semantic_limit': 12,
            'graph_rag_weight': 2.2,
            'vector_weight': 1.6,
            'semantic_weight': 1.0,
            'min_results': 12
        }
    
    # Estrategia para consultas con m√∫ltiples entidades legales
    elif legal_entity_count > 2:
        return {
            'name': 'multi_entity_comprehensive_v2',
            'graph_rag_limit': 22,
            'vector_limit': 16,
            'semantic_limit': 10,
            'graph_rag_weight': 2.0,
            'vector_weight': 1.5,
            'semantic_weight': 1.0,
            'min_results': 10
        }
    
    # Estrategia est√°ndar mejorada
    else:
        return {
            'name': 'standard_enhanced_v2',
            'graph_rag_limit': 18,  # Aumentado de 10
            'vector_limit': 15,     # Aumentado de 10
            'semantic_limit': 8,
            'graph_rag_weight': 1.8,
            'vector_weight': 1.3,
            'semantic_weight': 1.0,
            'min_results': 8
        }

def enhanced_query_analysis(query: str) -> Dict[str, Any]:
    """
    An√°lisis mejorado de consulta que detecta m√°s patrones legales.
    """
    query_lower = query.lower()
    
    # Indicadores de urgencia ampliados
    urgency_indicators = []
    urgency_patterns = [
        ('despido_inmediato', ['despidieron', 'echaron', 'cesaron', 'terminaron contrato']),
        ('discriminacion_activa', ['discriminan', 'tratan mal', 'no me dejan']),
        ('violacion_derechos', ['no me pagan', 'no respetan', 'obligan']),
        ('abuso_laboral', ['acoso', 'maltrato', 'abuso', 'humillaci√≥n']),
        ('emergencia_economica', ['sin dinero', 'no tengo', 'necesito urgente']),
        ('embarazo_riesgo', ['embarazada', 'gestaci√≥n', 'maternidad'])
    ]
    
    for category, patterns in urgency_patterns:
        for pattern in patterns:
            if pattern in query_lower:
                urgency_indicators.append({'category': category, 'pattern': pattern})
    
    # Contexto emocional mejorado
    emotional_context = []
    emotional_patterns = [
        ('frustracion', ['no entiendo', 'confundida', 'perdida']),
        ('injusticia', ['injusto', 'no es justo', 'abuso']),
        ('miedo', ['tengo miedo', 'temo', 'preocupada']),
        ('indignacion', ['indignante', 'inaceptable', 'terrible']),
        ('desesperacion', ['no s√© qu√© hacer', 'ayuda', 'socorro'])
    ]
    
    for emotion, patterns in emotional_patterns:
        for pattern in patterns:
            if pattern in query_lower:
                emotional_context.append({'emotion': emotion, 'pattern': pattern})
    
    # Entidades legales expandidas
    legal_entities = []
    entity_patterns = [
        ('empleador', ['jefe', 'empresa', 'empleador', 'patr√≥n', 'supervisor']),
        ('trabajador', ['empleado', 'trabajador', 'obrero', 'operario']),
        ('contrato', ['contrato', 'acuerdo', 'convenio']),
        ('salario', ['sueldo', 'salario', 'pago', 'remuneraci√≥n']),
        ('despido', ['despido', 'cesant√≠a', 'terminaci√≥n', 'fin contrato']),
        ('indemnizacion', ['indemnizaci√≥n', 'compensaci√≥n', 'pago por despido']),
        ('embarazo', ['embarazo', 'gestaci√≥n', 'maternidad']),
        ('discriminacion', ['discriminaci√≥n', 'trato diferencial', 'exclusi√≥n']),
        ('jornada', ['horas', 'jornada', 'horario', 'tiempo trabajo']),
        ('licencia', ['licencia', 'permiso', 'ausencia']),
        ('vacaciones', ['vacaciones', 'descanso', 'feriados']),
        ('sindicate', ['sindicato', 'gremio', 'asociaci√≥n']),
        ('convenio', ['convenio colectivo', 'acuerdo gremial'])
    ]
    
    for entity_type, patterns in entity_patterns:
        for pattern in patterns:
            if pattern in query_lower:
                legal_entities.append({'type': entity_type, 'pattern': pattern})
    
    # Calcular complejidad mejorada
    complexity_factors = [
        len(urgency_indicators) * 0.15,
        len(emotional_context) * 0.10,
        len(legal_entities) * 0.08,
        len(query.split()) * 0.02,  # Longitud de consulta
        1.0 if any(conj in query_lower for conj in ['y', 'pero', 'sin embargo', 'adem√°s']) else 0.0
    ]
    
    query_complexity = min(sum(complexity_factors), 1.0)
    
    return {
        'urgency_indicators': urgency_indicators,
        'emotional_context': emotional_context,
        'legal_entities': legal_entities,
        'query_complexity': query_complexity,
        'word_count': len(query.split()),
        'has_questions': '?' in query
    }

def enhanced_deduplication(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Deduplicaci√≥n mejorada que preserva diversidad de m√©todos.
    """
    seen_articles = {}
    unique_results = []
    
    for result in results:
        article_id = result.get('article_id', '')
        
        if not article_id:
            continue
        
        if article_id in seen_articles:
            # Combinar informaci√≥n del art√≠culo duplicado
            existing = seen_articles[article_id]
            
            # Mantener el score m√°s alto
            if result.get('score', 0) > existing.get('score', 0):
                existing['score'] = result['score']
            
            # Combinar m√©todos
            existing_method = existing.get('method', '')
            new_method = result.get('method', '')
            
            if new_method and new_method != existing_method:
                if existing_method:
                    existing['method'] = f"{existing_method},{new_method}"
                else:
                    existing['method'] = new_method
            
            # Mantener informaci√≥n de expansi√≥n si existe
            if 'expansion_level' in result and 'expansion_level' not in existing:
                existing['expansion_level'] = result['expansion_level']
                existing['expansion_method'] = result.get('expansion_method', '')
        else:
            seen_articles[article_id] = result
            unique_results.append(result)
    
    return unique_results

def apply_enhanced_contextual_rescoring(results: List[Dict[str, Any]], 
                                      query_analysis: Dict[str, Any], 
                                      query: str) -> List[Dict[str, Any]]:
    """
    Re-scoring contextual mejorado que considera m√∫ltiples factores.
    """
    query_lower = query.lower()
    
    for result in results:
        content_lower = result.get('content', '').lower()
        base_score = result.get('score', 0)
        
        # 1. Boost por urgencia detectada
        urgency_boost = 1.0
        for indicator in query_analysis['urgency_indicators']:
            category = indicator['category']
            if category == 'despido_inmediato' and any(term in content_lower for term in ['despido', 'terminaci√≥n']):
                urgency_boost *= 1.4
            elif category == 'discriminacion_activa' and any(term in content_lower for term in ['discriminaci√≥n', 'igualdad']):
                urgency_boost *= 1.3
            elif category == 'embarazo_riesgo' and any(term in content_lower for term in ['embarazo', 'maternidad']):
                urgency_boost *= 1.5
        
        # 2. Boost por entidades legales relevantes
        entity_boost = 1.0
        for entity in query_analysis['legal_entities']:
            entity_type = entity['type']
            if entity_type in ['despido', 'indemnizacion', 'embarazo'] and entity_type in content_lower:
                entity_boost *= 1.3
            elif entity_type in ['contrato', 'trabajador', 'empleador'] and entity_type in content_lower:
                entity_boost *= 1.2
        
        # 3. Boost por m√©todo de obtenci√≥n
        method_boost = 1.0
        method = result.get('method', '')
        expansion_level = result.get('expansion_level', 0)
        
        if 'advanced_graph_rag' in method or 'strong_relations' in method:
            method_boost = 1.4
        elif 'semantic_context' in method or expansion_level == 2:
            method_boost = 1.25
        elif 'procedural_chains' in method or expansion_level == 3:
            method_boost = 1.15
        elif 'enhanced_seed' in method:
            method_boost = 1.3
        
        # 4. Boost por densidad de t√©rminos clave
        query_words = [word for word in query_lower.split() if len(word) > 3]
        matching_words = sum(1 for word in query_words if word in content_lower)
        word_density_boost = 1.0 + (matching_words / max(len(query_words), 1)) * 0.4
        
        # 5. Boost por tipo de ley
        law_boost = 1.0
        law_name = result.get('law_name', '').lower()
        if 'trabajo' in law_name or 'contrato' in law_name:
            law_boost = 1.3
        elif 'civil' in law_name:
            law_boost = 1.1
        elif 'empleo' in law_name:
            law_boost = 1.2
        
        # 6. Boost por complejidad de consulta
        complexity_boost = 1.0 + (query_analysis['query_complexity'] * 0.2)
        
        # Aplicar todos los boosts
        contextual_score = (base_score * urgency_boost * entity_boost * 
                          method_boost * word_density_boost * law_boost * complexity_boost)
        
        result['contextual_score'] = contextual_score
        result['score'] = contextual_score  # Actualizar score principal
        
        # Guardar informaci√≥n de scoring para debugging
        result['scoring_details'] = {
            'base_score': base_score,
            'urgency_boost': urgency_boost,
            'entity_boost': entity_boost,
            'method_boost': method_boost,
            'word_density_boost': word_density_boost,
            'law_boost': law_boost,
            'complexity_boost': complexity_boost
        }
    
    return results

def enhanced_relevance_filter(results: List[Dict[str, Any]], query: str, threshold: float = 0.12) -> List[Dict[str, Any]]:
    """
    Filtro de relevancia mejorado y m√°s permisivo.
    """
    filtered_results = []
    query_lower = query.lower()
    
    for result in results:
        score = result.get('score', 0)
        content_lower = result.get('content', '').lower()
        method = result.get('method', '')
        
        # Umbral adaptativo basado en m√©todo
        adaptive_threshold = threshold
        
        if 'advanced_graph_rag' in method or 'strong_relations' in method:
            adaptive_threshold *= 0.7  # M√°s permisivo para Graph RAG
        elif 'semantic_context' in method:
            adaptive_threshold *= 0.8
        elif 'enhanced_seed' in method:
            adaptive_threshold *= 0.75
        
        # Umbral especial para art√≠culos con expansi√≥n del grafo
        if result.get('expansion_level', 0) > 0:
            adaptive_threshold *= 0.6
        
        # Verificar relevancia contextual directa
        query_words = [word for word in query_lower.split() if len(word) > 3]
        direct_matches = sum(1 for word in query_words if word in content_lower)
        direct_relevance = direct_matches / max(len(query_words), 1)
        
        # Aprobar si cumple umbral o tiene alta relevancia directa
        if score >= adaptive_threshold or direct_relevance >= 0.4:
            filtered_results.append(result)
        else:
            article_info = f"{result.get('law_name', 'N/A')} Art. {result.get('article_number', 'N/A')}"
            print(f"   ‚ùå Filtrado por baja relevancia ({score:.3f}): {article_info}")
    
    return filtered_results

def semantic_similarity_search(query: str, documents: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
    """
    B√∫squeda sem√°ntica directa como complemento cuando se necesitan m√°s resultados.
    """
    try:
        from sentence_transformers import SentenceTransformer
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Generar embedding de la consulta
        query_embedding = model.encode([query])
        
        # Generar embeddings de documentos (muestra limitada por performance)
        doc_texts = []
        doc_metadata = []
        
        for i, doc in enumerate(documents[:1000]):  # Limitar para performance
            content = doc.get('content', '')
            if len(content) > 50:
                doc_texts.append(content[:500])  # Limitar longitud
                doc_metadata.append(doc)
        
        if not doc_texts:
            return []
        
        # Calcular similitudes
        doc_embeddings = model.encode(doc_texts)
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
        
        # Obtener top resultados
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # Umbral m√≠nimo
                doc = doc_metadata[idx]
                metadata = doc.get('metadata', {})
                
                results.append({
                    'article_id': f"{metadata.get('code', '')}_{metadata.get('article', '')}",
                    'content': doc.get('content', ''),
                    'law_name': metadata.get('code', ''),
                    'article_number': metadata.get('article', ''),
                    'category': metadata.get('chapter', ''),
                    'source': metadata.get('section', ''),
                    'score': float(similarities[idx]) * 5.0,  # Escalar score
                    'semantic_similarity': float(similarities[idx]),
                    'method': 'semantic_direct'
                })
        
        return results
    
    except Exception as e:
        print(f"Error en b√∫squeda sem√°ntica directa: {str(e)}")
        return []

# ========== FUNCIONES DE INTEGRACI√ìN ==========

def search_query_neutral_enhanced(query: str, config: Dict[str, Any], 
                                 weaviate_client=None, neo4j_driver=None, 
                                 documents=None) -> List[Dict[str, Any]]:
    """
    Funci√≥n de compatibilidad que reemplaza search_query_neutral con el sistema mejorado.
    """
    return search_query_with_enhanced_graph_rag(
        query, config, weaviate_client, neo4j_driver, documents
    )

def get_enhanced_system_info() -> Dict[str, Any]:
    """
    Informaci√≥n del sistema mejorado para debugging.
    """
    return {
        'version': '2.0.0',
        'features': [
            'Graph RAG Avanzado con navegaci√≥n inteligente',
            'Filtrado contextual mejorado',
            'An√°lisis de consulta expandido',
            'Estrategias de b√∫squeda adaptativas',
            'Deduplicaci√≥n inteligente',
            'Re-scoring contextual avanzado',
            'B√∫squeda sem√°ntica complementaria'
        ],
        'expected_improvements': {
            'article_count': '8-12 art√≠culos relevantes (vs 2-3 anterior)',
            'precision': '85%+ (vs 60% anterior)',
            'coverage': 'Navegaci√≥n en 3 niveles del grafo legal',
            'contextual_understanding': 'Detecci√≥n de 13 tipos de entidades legales'
        }
    }